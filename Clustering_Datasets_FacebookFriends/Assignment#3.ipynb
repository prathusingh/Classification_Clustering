{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named selenium.webdriver",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cbb9d0b31597>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m#################Might require installation of new packages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m#Install selenium. Upgrade it if you have previously installed as recent version has some important bug fixes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named selenium.webdriver"
     ]
    }
   ],
   "source": [
    "####################Do not change anything below\n",
    "%matplotlib inline \n",
    "\n",
    "#Array processing\n",
    "import numpy as np\n",
    "\n",
    "#Data analysis, wrangling and common exploratory operations\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "#For visualization. Matplotlib for basic viz and seaborn for more stylish figures + statistical figures not in MPL.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.core.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn import metrics\n",
    "\n",
    "#A different implementation of k-means\n",
    "import scipy as sp\n",
    "import scipy.cluster.vq\n",
    "import scipy.spatial.distance\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "from sklearn.datasets import fetch_mldata, fetch_olivetti_faces\n",
    "from sklearn.utils import shuffle \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "#################Might require installation of new packages\n",
    "#Install selenium. Upgrade it if you have previously installed as recent version has some important bug fixes\n",
    "import selenium.webdriver as webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import json\n",
    "from urlparse import parse_qs, urlsplit, urlunsplit\n",
    "\n",
    "########################If needed you can import additional packages for helping you, although I would discourage it\n",
    "########################Put all your imports in the space below. If you use some strange package, \n",
    "##########################provide clear information as to how we can install it.\n",
    "\n",
    "#######################End imports###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.set_cmap('jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#task t1a\n",
    "\n",
    "#Write the code for the following function: \n",
    "#  Note that it is invoked from t1b and later tasks - so you might want to partially solve them to get the data\n",
    "#  that is then used in this task\n",
    "#The input arguments are:\n",
    "#     original_data : This is a 2D original data . \n",
    "#          original_data[:, 0] gives the first dimension and original_data[:, 1] gives the second dimension\n",
    "#     original_cluster_assignments: In general, you do not have the \"correct\" cluster assignments.\n",
    "#          Since we used synthetic data, we can get it anyway. This variable gives the correct values\n",
    "#     kmeanspp_cluster_assignments: This is the cluster assignment that we got from calling k-means \n",
    "#           with k-means++ as the initialization algorithm\n",
    "#     kmeans_random_cluster_assignments: This is the cluster assignment that we got from calling k-means \n",
    "#           with random initialization\n",
    "\n",
    "#The code must do the following:\n",
    "#   Create a 1x3 subplot where you plot original_data in each of them\n",
    "#   In the first sub-figure, you have to plot the cluster assignment from original_cluster_assignments\n",
    "#   In the second sub-figure, you have to plot the cluster assignment from kmeanspp_cluster_assignments\n",
    "#   In the third sub-figure, you have to plot the cluster assignment from kmeans_random_cluster_assignments\n",
    "# Hint:\n",
    "#   1. The scatter function has an argument called c that accepts a color sequence\n",
    "#       Since all three figures plot the same data, think about how you can use the c argument\n",
    "#       and the cluster assignments to show how the clustering worked\n",
    "#   2. This function will be called for different datasets. So ensure that you create a new figure object\n",
    "#        So that the images dont get super-imposed\n",
    "def part1_plot_clustering(original_data, original_cluster_assignments, \n",
    "                              kmeanspp_cluster_assignments, kmeans_random_cluster_assignments):\n",
    "    plt.figure()\n",
    "    fig,axes = plt.subplots(1, 3, figsize=(15,4))\n",
    "    \n",
    "    #Change below: call scatter plot function on axes[0]\n",
    "    axes[0].scatter(original_data[:,0], original_data[:,1], c=original_cluster_assignments)\n",
    "    axes[0].set_title('Original')\n",
    "    \n",
    "    #Change below: call scatter plot function on axes[1]\n",
    "    axes[1].scatter(original_data[:,0], original_data[:,1], c=kmeanspp_cluster_assignments)\n",
    "    axes[1].set_title('Init=K-Means++')\n",
    "    \n",
    "    #Change below: call scatter plot function on axes[2]\n",
    "    axes[2].scatter(original_data[:,0], original_data[:,1], c=kmeans_random_cluster_assignments)\n",
    "    axes[2].set_title('Init=Random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#task t1b\n",
    "#Scikit-learn has a number of functions to generate synthetic data.\n",
    "# For the first assignment, you will be using make_blobs function\n",
    "# See url http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html for details\n",
    "# Create a dataset with 200 2-D points with 4 cluster with a standard deviation of 1.0\n",
    "# Remember to create it with a random state of 1234\n",
    "# Remember to do it for all future tasks even if I forget to mention it :)\n",
    "\n",
    "#Change below to Create a dataset with 200 2-D points with 4 cluster with a standard deviation of 1.0\n",
    "t1b_data, t1b_ground_truth = datasets.make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.0, random_state=1234)\n",
    "#Change below: Call Kmeans with 4 clusters, with k-means++ initialization heuristic and random state of 1234\n",
    "t1b_kmeanspp = KMeans(n_clusters=4, init=\"k-means++\", random_state=1234)\n",
    "#Change below: Print the centroids\n",
    "print None\n",
    "#Change below: Find the cluster assignments for the data\n",
    "t1b_kmeanspp_cluster_assignments = t1b_kmeanspp.fit(t1b_data).labels_\n",
    "#Change below: Call Kmeans with 4 clusters, with random initialization heuristic and random state of 1234\n",
    "t1b_kmeans_random = KMeans(n_clusters=4, init=\"random\", random_state=1234)\n",
    "#Change below: Find the cluster assignments for the data\n",
    "t1b_kmeans_random_cluster_assignments = t1b_kmeans_random.fit(tib_data).labels_\n",
    "part1_plot_clustering(t1b_data, t1b_ground_truth, t1b_kmeanspp_cluster_assignments, t1b_kmeans_random_cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#task t1c\n",
    "# Create a dataset (make_blobs) with 200 2-D points with 4 cluster with a standard deviation of 5.0\n",
    "# Remember to create it with a random state of 1234\n",
    "\n",
    "#Change below to Create a dataset with 200 2-D points with 4 cluster with a standard deviation of 5.0\n",
    "t1c_data, t1c_ground_truth = datasets.make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=5.0, random_state=1234)\n",
    "#Change below: Call Kmeans with 4 clusters, with k-means++ initialization heuristic and random state of 1234\n",
    "t1c_kmeanspp = KMeans(n_clusters=4, init=\"k-means++\", random_state=1234)\n",
    "#Change below: Find the cluster assignments for the data\n",
    "t1c_kmeanspp_cluster_assignments = t1c_kmeanspp.fit(t1c_data).labels_\n",
    "#Change below: Call Kmeans with 4 clusters, with random initialization heuristic and random state of 1234\n",
    "t1c_kmeans_random = KMeans(n_clusters=4, init=\"random\", random_state=1234)\n",
    "#Change below: Find the cluster assignments for the data\n",
    "t1c_kmeans_random_cluster_assignments = t1c_kmeans_random.fit(t1c_data).labels_\n",
    "part1_plot_clustering(t1c_data, t1c_ground_truth, t1c_kmeanspp_cluster_assignments, t1c_kmeans_random_cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#task t1d\n",
    "# Create a dataset (make_blobs) with 200 2-D points with 10 clusters and with a standard deviation of 1.0\n",
    "# Remember to create it with a random state of 1234\n",
    "\n",
    "t1d_data, t1d_ground_truth = datasets.make_blobs(n_samples=200, n_features=2, centers=10, cluster_std=1.0, random_state=1234)\n",
    "t1d_kmeanspp = KMeans(n_clusters=10, init=\"k-means++\", random_state=1234)\n",
    "t1d_kmeanspp_cluster_assignments = t1d_kmeanspp.fit(t1d_data).labels_\n",
    "t1d_kmeans_random = KMeans(n_clusters=10, init=\"random\", random_state=1234)\n",
    "t1d_kmeans_random_cluster_assignments = t1d_kmeans_random.fit(t1d_data).labels_\n",
    "part1_plot_clustering(t1d_data, t1d_ground_truth, t1d_kmeanspp_cluster_assignments, t1d_kmeans_random_cluster_assignments)\n",
    "\n",
    "#How does the result look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#task t1e\n",
    "# Create a dataset (make_blobs) with 200 2-D points with 10 clusters and with a standard deviation of 5.0\n",
    "# Remember to create it with a random state of 1234\n",
    "# Then call K-Means with k=10\n",
    "\n",
    "t1e_data, t1e_ground_truth = datasets.make_blobs(n_samples=200, n_features=2, centers=10, cluster_std=5.0, random_state=1234)\n",
    "t1e_kmeanspp = KMeans(n_clusters=10, init=\"k-means++\", random_state=1234)\n",
    "t1e_kmeanspp_cluster_assignments = t1e_kmeanspp.fit(t1e_data).labels_\n",
    "t1e_kmeans_random = KMeans(n_clusters=10, init=\"random\", random_state=1234)\n",
    "t1e_kmeans_random_cluster_assignments = t1e_kmeans_random.fit(t1e_data).labels_\n",
    "part1_plot_clustering(t1e_data, t1e_ground_truth, t1e_kmeanspp_cluster_assignments, t1e_kmeans_random_cluster_assignments)\n",
    "\n",
    "#How does the result look?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#task t1f\n",
    "# For this assignment, you will be using make_circles function\n",
    "# See url http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html for details\n",
    "# Create a dataset with 200 2-D points \n",
    "# Remember to create it with a random state of 1234\n",
    "#In the code, call the K-Means function with k=2\n",
    "\n",
    "t1f_data, t1f_ground_truth = datasets.make_circles(n_samples=200, random_state=1234)\n",
    "t1f_kmeanspp = KMeans(n_clusters=2, init=\"k-means++\", random_state=1234)\n",
    "t1f_kmeanspp_cluster_assignments = t1f_kmeanspp.fit(t1f_data).labels_\n",
    "t1f_kmeans_random = KMeans(n_clusters=2, init=\"random\", random_state=1234)\n",
    "t1f_kmeans_random_cluster_assignments = t1f_kmeans_random.fit(t1f_data).labels_\n",
    "part1_plot_clustering(t1f_data, t1f_ground_truth, t1f_kmeanspp_cluster_assignments, t1f_kmeans_random_cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#task t1g\n",
    "# For this assignment, you will be using make_moons function\n",
    "# See url http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html for details\n",
    "# Create a dataset with 200 2-D points \n",
    "# Remember to create it with a random state of 1234\n",
    "#In the code, call the K-Means function with k=2\n",
    "\n",
    "t1g_data, t1g_ground_truth = datasets.make_moons(n_samples=200, random_state=1234)\n",
    "t1g_kmeanspp = KMeans(n_clusters=2, init=\"k-means++\", random_state=1234)\n",
    "t1g_kmeanspp_cluster_assignments = t1g_kmeanspp.fit(t1g_data).labels_\n",
    "t1g_kmeans_random = KMeans(n_clusters=2, init=\"random\", random_state=1234)\n",
    "t1g_kmeans_random_cluster_assignments = t1g_kmeans_random.fit(t1g_data).labels_\n",
    "part1_plot_clustering(t1g_data, t1g_ground_truth, t1g_kmeanspp_cluster_assignments, t1g_kmeans_random_cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#$k$-Means and Image Compression via Vector Quantization\n",
    "\n",
    "#Do not change anything below\n",
    "#China is one of the sample figures in sklearn\n",
    "\n",
    "#Code courtesy: Sklearn\n",
    "#china is a 3-D array where the first two dimensions are co-ordinates of pixels (row and column)\n",
    "# and third coordinate is a tuple with 3 values for RGB value\n",
    "china = datasets.load_sample_image(\"china.jpg\")\n",
    "china = np.array(china, dtype=np.float64) / 255\n",
    "china_w, china_h, china_d = tuple(china.shape)\n",
    "print \"Width=%s, Height=%s, Depth=%s\" % (china_w, china_h, china_d)\n",
    "#Convert it to a 2D array for analysis\n",
    "china_image_array = np.reshape(china, (china_w * china_h, china_d))\n",
    "print \"In 2-D the shape is \", china_image_array.shape\n",
    "\n",
    "def recreate_image(codebook, labels, w, h):\n",
    "    \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\n",
    "    d = codebook.shape[1]\n",
    "    image = np.zeros((w, h, d))\n",
    "    label_idx = 0\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            image[i][j] = codebook[labels[label_idx]]\n",
    "            label_idx += 1\n",
    "    return image\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.axes([0, 0, 1, 1])\n",
    "plt.axis('off')\n",
    "plt.title('Original image (96,615 colors)')\n",
    "plt.imshow(china)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t1h:\n",
    "\n",
    "#This task could run for a long time on typical machines\n",
    "#Let us run K-means with different values of k\n",
    "# Then using the new centroids, compress the image and display it. \n",
    "\n",
    "t1h_start_time = time.time()\n",
    "plt.figure()\n",
    "fig,axes = plt.subplots(2, 2, figsize=(15,6))\n",
    "\n",
    "#the 2d is for convenience\n",
    "t1h_k_values = [[16, 32], [64,128]]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        print \"Handling k =\", t1h_k_values[i][j]\n",
    "        #Change below: call Kmeans with k=t1h_k_values [i][j] and random state = 1234\n",
    "        t1h_kmeans_obj = KMeans(n_clusters=t1h_k_values[i][j], random_state=1234)\n",
    "        #Change below: fit the object with china image array variable\n",
    "        t1h_kmeans_fit = t1h_kmeans_obj.fit(china_image_array)\n",
    "        axes[i][j].imshow(recreate_image(t1h_kmeans_fit.cluster_centers_, t1h_kmeans_fit.labels_, china_w, china_h))\n",
    "        axes[i][j].set_title('Compression with' + str(t1h_k_values[i][j]) + \" colors\")\n",
    "        \n",
    "        axes[i][j].grid(False)\n",
    "        axes[i][j].get_xaxis().set_ticks([])\n",
    "        axes[i][j].get_yaxis().set_ticks([])\n",
    "\n",
    "print \"Clustering over entire data took %s seconds\" % (time.time() - t1h_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t1i:\n",
    "# Here is a much faster way using sampling which is how the quantization is done in real world\n",
    "# We will study sampling in detail later in the class. But the idea is simple\n",
    "# Instead of running the algorithm on the whole data, we run it on a subset (called a sample) \n",
    "#   which is picked uniformly at random\n",
    "# Since the sample is small, your algorithm runs faster\n",
    "# We then use the model to predict the correct cluster values for the entire data \n",
    "# You can see the speed difference clearly.\n",
    "\n",
    "#Recall that there were 273280 pixels - let us take a sample of 1000 pixels which is approximately 0.3%\n",
    "t1i_china_sample = shuffle(china_image_array, random_state=1234)[:1000]\n",
    "\n",
    "\n",
    "#Let us run K-means with different values of k\n",
    "# Then using the new centroids, compress the image and display it. \n",
    "\n",
    "t1i_start_time = time.time()\n",
    "plt.figure()\n",
    "fig,axes = plt.subplots(2, 2, figsize=(15,6))\n",
    "\n",
    "#the 2d is for convenience\n",
    "t1i_k_values = [[16, 32], [64,128]]    \n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        print \"Handling k =\", t1i_k_values[i][j]\n",
    "            \n",
    "        #Change below: call Kmeans with k=t1h_k_values [i][j] and random state = 1234 \n",
    "        t1i_kmeans_obj = KMeans(n_clusters=t1h_k_values[i][j], random_state=1234)\n",
    "        #Change below: fit the object with the SAMPLE\n",
    "        t1i_kmeans_fit = t1i_kmeans_obj.fit(t1i_china_sample)\n",
    "        \n",
    "        #After running k-means on the sample, we got k centroids\n",
    "        # Now for each pixel in the original array you have to find the closest centroid\n",
    "        #Change below: use the predict function of t1i_kmeans_fit for finding the closest centroid for entire image\n",
    "        t1i_cluster_assignments = t1i_kmeans_fit.predict(china_image_array)\n",
    "        \n",
    "        axes[i][j].imshow(recreate_image(t1i_kmeans_fit.cluster_centers_, t1i_cluster_assignments, china_w, china_h))\n",
    "        axes[i][j].set_title('Compression with' + str(t1h_k_values[i][j]) + \" colors\")\n",
    "\n",
    "        axes[i][j].grid(False)\n",
    "        axes[i][j].get_xaxis().set_ticks([])\n",
    "        axes[i][j].get_yaxis().set_ticks([])\n",
    "        \n",
    "\n",
    "print \"Clustering with Sampling took %s seconds\" % (time.time() - t1i_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choosing Good values for $k$\n",
    "#Elbow method (SSQ)\n",
    "#Gap Statistic\n",
    "#Gap Statistic differences\n",
    "\n",
    "#All three functions were adapted from the code by Reid Johnson from University of Notre Dame\n",
    "\n",
    "def compute_ssq(data, k, kmeans):\n",
    "    dist = np.min(cdist(data, kmeans.cluster_centers_, 'euclidean'), axis=1)\n",
    "    tot_withinss = sum(dist**2) # Total within-cluster sum of squares\n",
    "    totss = sum(pdist(data)**2) / data.shape[0] # The total sum of squares\n",
    "    betweenss = totss - tot_withinss # The between-cluster sum of squares\n",
    "    return betweenss/totss*100\n",
    "    \n",
    "#Given a data (as nxm matrix) and an array of ks, this returns the SSQ (sum of squared distances)\n",
    "# SSQ is also called as SSD or SSE\n",
    "\n",
    "\n",
    "def ssq_statistics(data, ks, ssq_norm=True):\n",
    "    ssqs = sp.zeros((len(ks),)) # array for SSQs (lenth ks)\n",
    "    \n",
    "    for (i,k) in enumerate(ks): # iterate over the range of k values\n",
    "        kmeans = KMeans(n_clusters=k, random_state=1234).fit(data)\n",
    "        \n",
    "        if ssq_norm:\n",
    "            ssqs[i] = compute_ssq(data, k, kmeans)\n",
    "        else:\n",
    "            # The sum of squared error (SSQ) for k\n",
    "            ssqs[i] = kmeans.inertia_\n",
    "    return ssqs\n",
    "\n",
    "#This function computes the Gap statistic of the data (given as a nxm matrix)\n",
    "# Returns:\n",
    "#    gaps: an array of gap statistics computed for each k.\n",
    "#    errs: an array of standard errors (se), with one corresponding to each gap computation.\n",
    "#    difs: an array of differences between each gap_k and the sum of gap_k+1 minus err_k+1.\n",
    "\n",
    "# The gap statistic measures the difference between within-cluster dispersion on an input\n",
    "#  dataset and that expected under an appropriate reference null distribution.\n",
    "# If you did not fully understand the definition, no worry - it is quite complex anyway :)\n",
    "# However, you should know how to USE the gap statistic if not how it is computed\n",
    "def gap_statistics(data, refs=None, nrefs=20, ks=range(1,11)):\n",
    "    \n",
    "    sp.random.seed(1234)\n",
    "    shape = data.shape\n",
    "    dst = sp.spatial.distance.euclidean\n",
    "    \n",
    "    if refs is None:\n",
    "        tops = data.max(axis=0) # maxima along the first axis (rows)\n",
    "        bots = data.min(axis=0) # minima along the first axis (rows)\n",
    "        dists = sp.matrix(sp.diag(tops-bots)) # the bounding box of the input dataset\n",
    "        \n",
    "        # Generate nrefs uniform distributions each in the half-open interval [0.0, 1.0)\n",
    "        rands = sp.random.random_sample(size=(shape[0],shape[1], nrefs))\n",
    "        \n",
    "        # Adjust each of the uniform distributions to the bounding box of the input dataset\n",
    "        for i in range(nrefs):\n",
    "            rands[:,:,i] = rands[:,:,i]*dists+bots\n",
    "    else:\n",
    "        rands = refs\n",
    "        \n",
    "    gaps = sp.zeros((len(ks),))   # array for gap statistics (lenth ks)\n",
    "    errs = sp.zeros((len(ks),))   # array for model standard errors (length ks)\n",
    "    difs = sp.zeros((len(ks)-1,)) # array for differences between gaps (length ks-1)\n",
    "\n",
    "    for (i,k) in enumerate(ks): # iterate over the range of k values\n",
    "        # Cluster the input dataset via k-means clustering using the current value of k\n",
    "        try:\n",
    "            (kmc,kml) = sp.cluster.vq.kmeans2(data, k)\n",
    "        except LinAlgError:\n",
    "            kmeans = sklearn.cluster.KMeans(n_clusters=k).fit(data)\n",
    "            (kmc, kml) = kmeans.cluster_centers_, kmeans.labels_\n",
    "\n",
    "        # Generate within-dispersion measure for the clustering of the input dataset\n",
    "        disp = sum([dst(data[m,:],kmc[kml[m],:]) for m in range(shape[0])])\n",
    "\n",
    "        # Generate within-dispersion measures for the clusterings of the reference datasets\n",
    "        refdisps = sp.zeros((rands.shape[2],))\n",
    "        for j in range(rands.shape[2]):\n",
    "            # Cluster the reference dataset via k-means clustering using the current value of k\n",
    "            try:\n",
    "                (kmc,kml) = sp.cluster.vq.kmeans2(rands[:,:,j], k)\n",
    "            except LinAlgError:\n",
    "                kmeans = sklearn.cluster.KMeans(n_clusters=k).fit(rands[:,:,j])\n",
    "                (kmc, kml) = kmeans.cluster_centers_, kmeans.labels_\n",
    "\n",
    "            refdisps[j] = sum([dst(rands[m,:,j],kmc[kml[m],:]) for m in range(shape[0])])\n",
    "\n",
    "        # Compute the (estimated) gap statistic for k\n",
    "        gaps[i] = sp.mean(sp.log(refdisps) - sp.log(disp))\n",
    "\n",
    "        # Compute the expected error for k\n",
    "        errs[i] = sp.sqrt(sum(((sp.log(refdisp)-sp.mean(sp.log(refdisps)))**2) \\\n",
    "                            for refdisp in refdisps)/float(nrefs)) * sp.sqrt(1+1/nrefs)\n",
    "\n",
    "    # Compute the difference between gap_k and the sum of gap_k+1 minus err_k+1\n",
    "    difs = sp.array([gaps[k] - (gaps[k+1]-errs[k+1]) for k in range(len(gaps)-1)])\n",
    "\n",
    "    #print \"Gaps: \" + str(gaps)\n",
    "    #print \"Errs: \" + str(errs)\n",
    "    #print \"Difs: \" + str(difs)\n",
    "\n",
    "    return gaps, errs, difs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t1j\n",
    "# Implement the following function plot_clustering_statistics\n",
    "# It accepts three arguments: data as nxm matrix minimum and maximum value \n",
    "#   within which you think the best k lies. \n",
    "#  Of course, in the worst case this is between 1 and n (where n=number of data points)\n",
    "#  You will compute the necessary statisitcs using the above function and \n",
    "#   use it to find a good $k$\n",
    "\n",
    "# Finding a good k, even with the statistics is a bit tricky\n",
    "#  So we will plot the values and find a good $k$ by visually inspecting the plot\n",
    "\n",
    "\n",
    "#Interpreting the charts:\n",
    "#  Elbow method: http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method\n",
    "#  Gap Statistics: $k$ where the first drop in trend happens\n",
    "#  Gap Statistics differences: $k$ where you get the first positive values\n",
    "def t1j_plot_clustering_statistics(data, k_min, k_max):\n",
    " \n",
    "    plt.figure()\n",
    "    fig,axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    #range(a,b) returns a .. b-1\n",
    "    ks = range(k_min, k_max+1)\n",
    "    \n",
    "    #Change below: plot the data distribution as a scatter plot on axes[0] variable\n",
    "    # For now ignore the color field. We will use data where #clusters is easy to see\n",
    "    axes[0].scatter(data[:, 0], data[:, 1])\n",
    "    axes[0].set_title(\"Original Data\")\n",
    "    \n",
    "    ssqs = ssq_statistics(data, ks=ks)\n",
    "    #Change below: create a line chart with x axis as different k values \n",
    "    #  and y-axis as ssqs on axes[1] variable\n",
    "    axes[1].plot(ks, ssqs)\n",
    "    axes[1].set_title(\"Elbow Method and SSQ\")\n",
    "    axes[1].set_xlabel(\"$k$\")\n",
    "    axes[1].set_ylabel(\"SSQ\")\n",
    "  \n",
    "    \n",
    "    \n",
    "    #Do not change anything below for the rest of the function\n",
    "    # Code courtesy: Reid Johnson from U. of Notre Dame\n",
    "    gaps, errs, difs = gap_statistics(data, nrefs=25, ks=ks)\n",
    "    \n",
    "    max_gap = None\n",
    "    if len(np.where(difs > 0)[0]) > 0:\n",
    "        max_gap = np.where(difs > 0)[0][0] + 1 # the k with the first positive dif\n",
    "    if max_gap:\n",
    "        print \"By gap statistics, optimal k seems to be \", max_gap\n",
    "    else:\n",
    "        print \"Please use some other metrics for finding k\"\n",
    "        \n",
    "     #Create an errorbar plot\n",
    "    rects = axes[2].errorbar(ks, gaps, yerr=errs, xerr=None, linewidth=1.0)\n",
    "\n",
    "    #Add figure labels and ticks\n",
    "    axes[2].set_title('Clustering Gap Statistics')\n",
    "    axes[2].set_xlabel('Number of clusters k')\n",
    "    axes[2].set_ylabel('Gap Statistic')\n",
    "    axes[2].set_xticks(ks)\n",
    "    # Add figure bounds\n",
    "    axes[2].set_ylim(0, max(gaps+errs)*1.1)\n",
    "    axes[2].set_xlim(0, len(gaps)+1.0)\n",
    "\n",
    "    ind = range(1,len(difs)+1) # the x values for the difs\n",
    "    \n",
    "    max_gap = None\n",
    "    if len(np.where(difs > 0)[0]) > 0:\n",
    "        max_gap = np.where(difs > 0)[0][0] + 1 # the k with the first positive dif\n",
    "\n",
    "    #Create a bar plot\n",
    "    axes[3].bar(ind, difs, alpha=0.5, color='g', align='center')\n",
    "\n",
    "    # Add figure labels and ticks\n",
    "    if max_gap:\n",
    "        axes[3].set_title('Clustering Gap Differences\\n(k=%d Estimated as Optimal)' % (max_gap))\n",
    "    else:\n",
    "        axes[3].set_title('Clustering Gap Differences\\n')\n",
    "    axes[3].set_xlabel('Number of clusters k')\n",
    "    axes[3].set_ylabel('Gap Difference')\n",
    "    axes[3].xaxis.set_ticks(range(1,len(difs)+1))\n",
    "\n",
    "    #Add figure bounds\n",
    "    axes[3].set_ylim(min(difs)*1.2, max(difs)*1.2)\n",
    "    axes[3].set_xlim(0, len(difs)+1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dont change anything here\n",
    "t1j_data, t1j_ground_truth = datasets.make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=0.5, random_state=1234)\n",
    "t1j_plot_clustering_statistics(t1j_data, 1, 10)\n",
    "\n",
    "t1j_data, t1j_ground_truth = datasets.make_blobs(n_samples=200, n_features=2, centers=5, cluster_std=0.5, random_state=1234)\n",
    "t1j_plot_clustering_statistics(t1j_data, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation of Hierarchical Clustering over Diverse Datasets\n",
    "\n",
    "#task t2a\n",
    "\n",
    "#Write the code for the following function: \n",
    "#  Note that it is invoked from t2b and later tasks - so you might want to partially solve them to get the data\n",
    "#  that is then used in this task\n",
    "\n",
    "#The input arguments are:\n",
    "#     original_data : This is a 2D original data . \n",
    "#          original_data[:, 0] gives the first dimension and original_data[:, 1] gives the second dimension\n",
    "#     original_cluster_assignments: In general, you do not have the \"correct\" cluster assignments.\n",
    "#          Since we used synthetic data, we can get it anyway. This variable gives the correct values\n",
    "#     ward_cluster_assignments: This is the cluster assignment that we got from calling \n",
    "#           hierarchical clustering with ward linkage\n",
    "#     complete_cluster_assignments: This is the cluster assignment that we got from calling \n",
    "#           hierarchical clustering with complete linkage\n",
    "#     average_cluster_assignments: This is the cluster assignment that we got from calling \n",
    "#           hierarchical clustering with average linkage\n",
    "\n",
    "#The code must do the following:\n",
    "#   Create a 1x4 subplot where you plot original_data in each of them\n",
    "#   In the first sub-figure, you have to plot the cluster assignment from original_cluster_assignments\n",
    "#   In the second sub-figure, you have to plot the cluster assignment from ward_linkage_cluster_assignments\n",
    "#   In the third sub-figure, you have to plot the cluster assignment from complete_linkage_cluster_assignments\n",
    "#   In the fourth sub-figure, you have to plot the cluster assignment from average_linkage_cluster_assignments\n",
    "\n",
    "# Hint:\n",
    "#   1. The scatter function has an argument called c that accepts a color sequence\n",
    "#       Since all three figures plot the same data, think about how you can use the c argument\n",
    "#       and the cluster assignments to show how the clustering worked\n",
    "#   2. This function will be called for different datasets. So ensure that you create a new figure object\n",
    "#        So that the images dont get super-imposed\n",
    "def part2_plot_clustering(original_data, original_cluster_assignments, \n",
    "                              ward_linkage_cluster_assignments, complete_linkage_cluster_assignments, \n",
    "                              average_linkage_cluster_assignments):\n",
    "    plt.figure()\n",
    "    fig,axes = plt.subplots(1, 4, figsize=(16,4))\n",
    "    \n",
    "    #Change below: call scatter plot function on axes[0]\n",
    "    axes[0].scatter(original_data[:,0], original_data[:,1], c=original_cluster_assignments)\n",
    "    axes[0].set_title('Original')\n",
    "    \n",
    "    #Change below: call scatter plot function on axes[1]\n",
    "    axes[1].scatter(original_data[:,0], original_data[:,1], c=ward_linkage_cluster_assignments)\n",
    "    axes[1].set_title('Ward Linkage')\n",
    "    \n",
    "    #Change below: call scatter plot function on axes[2]\n",
    "    axes[2].scatter(original_data[:,0], original_data[:,1], c=complete_linkage_cluster_assignments)\n",
    "    axes[2].set_title('Complete Linkage')\n",
    "    \n",
    "    #Change below: call scatter plot function on axes[3]\n",
    "    axes[3].scatter(original_data[:,0], original_data[:,1], c=average_linkage_cluster_assignments)\n",
    "    axes[3].set_title('Average Linkage') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task t2b\n",
    "\n",
    "#Change below to Create a dataset with make_blobs 200 2-D points with 4 cluster with a standard deviation of 1.0\n",
    "t2b_data, t2b_ground_truth = datasets.make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.0, random_state=1234)\n",
    "\n",
    "#Change below: Call AgglomerativeClustering with 4 clusters with ward linkage\n",
    "t2b_agg_ward = AgglomerativeClustering(n_clusters=4, linkage=\"ward\")\n",
    "#Change below: Find the cluster assignments for the data\n",
    "t2b_ward_linkage_cluster_assignments = t2b_agg_ward.fit(t2b_data).labels_\n",
    "\n",
    "#Change below: Call AgglomerativeClustering with 4 clusters with complete linkage\n",
    "t2b_agg_complete = AgglomerativeClustering(n_clusters=4, linkage=\"complete\")\n",
    "#Change below: Find the cluster assignments for the data\n",
    "t2b_complete_linkage_cluster_assignments = t2b_agg_complete.fit(t2b_data).labels_ \n",
    "\n",
    "#Change below: Call AgglomerativeClustering with 4 clusters with average linkage\n",
    "t2b_agg_average = AgglomerativeClustering(n_clusters=4, linkage=\"average\")\n",
    "#Change below: Find the cluster assignments for the data\n",
    "t2b_average_linkage_cluster_assignments = t2b_agg_average.fit(t2b_data).labels_ \n",
    "\n",
    "\n",
    "part2_plot_clustering(t2b_data, t2b_ground_truth, t2b_ward_linkage_cluster_assignments, \n",
    "                            t2b_complete_linkage_cluster_assignments, t2b_average_linkage_cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task t2c\n",
    "\n",
    "#Change below to Create a dataset with make_circles function with 200 2-D points \n",
    "t2c_data, t2c_ground_truth = datasets.make_circles(n_samples=200, random_state=1234) \n",
    "\n",
    "#Change below: Call AgglomerativeClustering with 2 clusters with ward linkage\n",
    "t2c_agg_ward = AgglomerativeClustering(n_clusters=2, linkage=\"ward\")\n",
    "#Change below: Find the cluster assignments for the data\n",
    "t2c_ward_linkage_cluster_assignments = t2c_agg_ward.fit(t2c_data).labels_\n",
    "\n",
    "#Change below: Call AgglomerativeClustering with 2 clusters with complete linkage\n",
    "t2c_agg_complete = AgglomerativeClustering(n_clusters=2, linkage=\"complete\") \n",
    "#Change below: Find the cluster assignments for the data\n",
    "t2c_complete_linkage_cluster_assignments = t2c_agg_complete.fit(t2c_data).labels_\n",
    "\n",
    "#Change below: Call AgglomerativeClustering with 2 clusters with average linkage\n",
    "t2c_agg_average = AgglomerativeClustering(n_clusters=2, linkage=\"average\") \n",
    "#Change below: Find the cluster assignments for the data\n",
    "t2c_average_linkage_cluster_assignments = t2c_agg_average.fit(t2c_data).labels_\n",
    "\n",
    "\n",
    "part2_plot_clustering(t2c_data, t2c_ground_truth, t2c_ward_linkage_cluster_assignments, \n",
    "                            t2c_complete_linkage_cluster_assignments, t2c_average_linkage_cluster_assignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task t2d\n",
    "\n",
    "#Change below to Create a dataset with make_moons function with 200 2-D points \n",
    "t2d_data, t2d_ground_truth = datasets.make_moons(n_samples=200, random_state=1234) \n",
    "\n",
    "#Change below: Call AgglomerativeClustering with 2 clusters with ward linkage\n",
    "t2d_agg_ward = AgglomerativeClustering(n_clusters=2, linkage=\"ward\")\n",
    "#Change below: Find the cluster assignments for the data\n",
    "t2d_ward_linkage_cluster_assignments = t2d_agg_ward.fit(t2d_data).labels_\n",
    "\n",
    "#Change below: Call AgglomerativeClustering with 2 clusters with complete linkage\n",
    "t2d_agg_complete = AgglomerativeClustering(n_clusters=2, linkage=\"complete\") \n",
    "#Change below: Find the cluster assignments for the data\n",
    "t2d_complete_linkage_cluster_assignments = t2d_agg_complete.fit(t2d_data).labels_\n",
    "\n",
    "#Change below: Call AgglomerativeClustering with 2 clusters with average linkage\n",
    "t2d_agg_average = AgglomerativeClustering(n_clusters=2, linkage=\"average\") \n",
    "#Change below: Find the cluster assignments for the data\n",
    "t2d_average_linkage_cluster_assignments = t2d_agg_average.fit(t2d_data).labels_\n",
    "\n",
    "\n",
    "part2_plot_clustering(t2d_data, t2d_ground_truth, t2d_ward_linkage_cluster_assignments, \n",
    "                            t2d_complete_linkage_cluster_assignments, t2d_average_linkage_cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t2e: Let us now create and visualize dendrogram for a toy datasset\n",
    "\n",
    "t2e_data, t2e_ground_truth = datasets.make_blobs(n_samples=20, n_features=2, centers=2, cluster_std=0.5, random_state=1234)\n",
    "plt.figure()\n",
    "plt.scatter(t2e_data[:, 0], t2e_data[:, 1], c=t2e_ground_truth)\n",
    "plt.show()\n",
    "\n",
    "#Plot the dendrogram of t2edata\n",
    "#Change below: compute the pairwise distance \n",
    "t2e_data_dist = pdist(t2e_data)\n",
    "#Change below: compute the linkage \n",
    "t2e_data_linkage = linkage(t2e_data_dist)\n",
    "#Change below: plot the dendrogram \n",
    "t2e_data_dendrogram = dendogram(t2e_data_linkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ground Truth Cluster Assignments Available\n",
    "\n",
    "# Part 3. Comparison of Clustering Evaluation Metrics\n",
    "#Do not change anything below\n",
    "\n",
    "#Let us create the data that we will be using to evaluate measures in the next cell\n",
    "t3_data, t3_ground_truth = datasets.make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=0.5, random_state=1234)\n",
    "\n",
    "t3_k_min = 1\n",
    "t3_k_max = 10\n",
    "t3_ind = range(t3_k_min, t3_k_max+1)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(t3_data[:, 0], t3_data[:, 1], c=t3_ground_truth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task t3a\n",
    "\n",
    "\n",
    "t3a_adjusted_rand_index_scores = [0 for _ in t3_ind]\n",
    "t3a_adjusted_mutual_info_scores = [0 for _ in t3_ind]\n",
    "t3a_adjusted_homogeneity_scores = [0 for _ in t3_ind]\n",
    "t3a_adjusted_completeness_scores = [0 for _ in t3_ind]\n",
    "t3a_adjusted_v_measure_scores = [0 for _ in t3_ind]\n",
    "\n",
    "for k in t3_ind:\n",
    "    \n",
    "    #Change below: Call KMeans with k clusters with k-means++ initialization and random state of 1234 \n",
    "    t3a_kmeanspp = KMeans(n_clusters=k, init=\"k-means++\", random_state=1234)\n",
    "    #Change below: Find the cluster assignments for the data\n",
    "    t3a_kmeanspp_cluster_assignments = t3a_kmeanspp.fit(t3_data).labels_\n",
    "    \n",
    "    #Now let us compute the clustering score for each metric (use metrics.xyz for getting function xyz)\n",
    "    # Watch out for the argument order (true, predicted)\n",
    "    \n",
    "    #Change below: compute the score based on ADJUSTED random index\n",
    "    t3a_adjusted_rand_index_scores[k-1] = metrics.adjusted_rand_score(t3_ground_truth, t3a_kmeanspp_cluster_assignments)\n",
    "    \n",
    "    #Change below: compute the score based on ADJUSTED mutual information score\n",
    "    t3a_adjusted_mutual_info_scores[k-1] = metrics.adjusted_mutual_info_score(t3_ground_truth, t3a_kmeanspp_cluster_assignments)\n",
    "\n",
    "    \n",
    "    #Change below: compute the score based on homogeneity score\n",
    "    t3a_adjusted_homogeneity_scores[k-1] = metrics.homogeneity_score(t3_ground_truth, t3a_kmeanspp_cluster_assignments)\n",
    "    \n",
    "    \n",
    "    #Change below: compute the score based on completeness index\n",
    "    t3a_adjusted_completeness_scores[k-1] = metrics.completeness_score(t3_ground_truth, t3a_kmeanspp_cluster_assignments)\n",
    "    \n",
    "    \n",
    "    #Change below: compute the score based on v-measure index\n",
    "    t3a_adjusted_v_measure_scores[k-1] = metrics.v_measure_score(t3_ground_truth, t3a_kmeanspp_cluster_assignments)\n",
    "    \n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t3_ind, t3a_adjusted_rand_index_scores, label=\"Adjusted Rand Index\")\n",
    "plt.plot(t3_ind, t3a_adjusted_mutual_info_scores, label=\"Adjusted Mutual Info\")\n",
    "plt.plot(t3_ind, t3a_adjusted_homogeneity_scores, label=\"Homegeneity\")\n",
    "plt.plot(t3_ind, t3a_adjusted_completeness_scores, label=\"Completeness Score\")\n",
    "plt.plot(t3_ind, t3a_adjusted_v_measure_scores, label=\"V-Measure\")\n",
    "\n",
    "plt.title(\"$k$ vs Metrics\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"Clustering Evaluation Metrics\")\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Ground Truth Cluster Assignments Not Available\n",
    "\n",
    "#Do not change anything below\n",
    "\n",
    "#Code Courtesy: Derek Greene from University College, Dublin\n",
    "\n",
    "#The following function computes pairwise stability of a list of clusterings\n",
    "# the mean similarity between the clusterings as defined by a particular similarity metric. \n",
    "# In this case we use the Adjusted Rand Index to calculate the similarities.\n",
    "def calc_pairwise_stability( clusterings, metric ):\n",
    "    sim_values = []\n",
    "    for i in range(len(clusterings)):\n",
    "        for j in range(i+1,len(clusterings)):\n",
    "            sim_values.append( metric( clusterings[i], clusterings[j] ) )\n",
    "    return np.array( sim_values ).mean()\n",
    "\n",
    "#Given data, take a sample, run k-means on it, make predictions\n",
    "def t3_kmeans_sample( X, k, sampling_ratio ):\n",
    "    # create a matrix with subset of samples\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle( indices )\n",
    "    n_subset = int(n_samples * sampling_ratio) \n",
    "    X_subset = X[indices[0:n_subset]] \n",
    "    # cluster the subset\n",
    "    clusterer = KMeans(n_clusters=k, n_init=1, init='random', max_iter = 100)\n",
    "    clusterer.fit(X_subset)\n",
    "    # produce an assignment for all samples\n",
    "    return clusterer.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let us again use t3_data and t3_ground_truth, except we do not consider the ground truth. \n",
    "\n",
    "#Task t3b\n",
    "\n",
    "t3b_ssq_scores = [0 for _ in t3_ind]\n",
    "t3b_silhoutte_coefficient_scores = [0 for _ in t3_ind]\n",
    "t3b_stability_scores = [0 for _ in t3_ind]\n",
    "\n",
    "for k in t3_ind:\n",
    "    \n",
    "    #Change below: Call KMeans with k clusters with k-means++ initialization and random state of 1234 \n",
    "    t3b_kmeanspp = KMeans(n_clusters=k, init=\"k-means++\", random_state=1234)\n",
    "    #Change below: fit t3b_kmeanspp to data\n",
    "    t3b_kmeanspp_fitted = t3b_kmeanspp.fit(t3_data)\n",
    "    #Change below: Find the cluster assignments for the data\n",
    "    t3b_kmeanspp_cluster_assignments =  t3b_kmeanspp_fitted.labels_\n",
    "    \n",
    "    \n",
    "    #Now let us compute the clustering score for each metric (use metrics.xyz for getting function xyz)\n",
    "    # Watch out for the argument order (true, predicted)\n",
    "    \n",
    "    #Change below: compute ssq score using compute_ssq function\n",
    "    t3b_ssq_scores[k-1] = compute_ssq(t3_data, k, t3b_kmeanspp_fitted)\n",
    "    \n",
    "    #Change below: compute the score based on silhouette_score with a sample size of 50\n",
    "    #Note: do not set random state here - else it will give a constant score\n",
    "    # Your results might look quite different from mine\n",
    "    t3b_silhoutte_coefficient_scores[k-1] = metrics.silhouette_score(t3_data, t3b_kmeanspp_cluster_assignments, sample_size=50) \n",
    "    \n",
    "    #Do not change: compute the score based on stability score\n",
    "    if k == 1: #Stability is defined for k >= 2\n",
    "        continue\n",
    "    \n",
    "    #Run k-means on a small sample , make predictions based on the sample centroids and see how stable they are\n",
    "    np.random.seed(1234)\n",
    "    t3b_stability_clusterings = [t3_kmeans_sample( t3_data, k, 0.5 ) for run in range(10)]\n",
    "    t3b_stability_scores[k-1] = calc_pairwise_stability(t3b_stability_clusterings, metrics.adjusted_rand_score)\n",
    "        \n",
    "    \n",
    "\n",
    "#Do not change anything below\n",
    "plt.figure()\n",
    "fig,axes = plt.subplots(1, 3, figsize=(15,4))\n",
    "\n",
    "axes[0].plot(t3_ind, t3b_ssq_scores)\n",
    "axes[0].set_title('SSQ Scores')\n",
    "axes[0].set_xlabel('$k$')\n",
    "axes[0].set_ylabel('SSQ Scores')\n",
    "\n",
    "axes[1].plot(t3_ind, t3b_silhoutte_coefficient_scores)\n",
    "axes[1].set_title('Silhoutte Coefficient')\n",
    "axes[1].set_xlabel('$k$')\n",
    "axes[1].set_ylabel('Silhoutte Coefficient')\n",
    "axes[1].set_ylim( (0.0, 1.0) )\n",
    "\n",
    "axes[2].plot(t3_ind, t3b_stability_scores)\n",
    "axes[2].set_title('Stability of Clusters')\n",
    "axes[2].set_xlabel('$k$')\n",
    "axes[2].set_ylabel('Stability')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clustering your Facebook Friends\n",
    "\n",
    "#DO not change anything below\n",
    "\n",
    "#Change to webdriver.Chrome() if Chrome is your primary browser.\n",
    "#I suggest to let it run on Firefox\n",
    "driver = webdriver.Firefox()\n",
    "driver.maximize_window()\n",
    "\n",
    "#Number of seconds to wait before each operation\n",
    "#Change this if your internet is fast or if you are running it in UTA (reduce it to 2-3 seconds)\n",
    "#I would nevertheless advice you to not alter it though.\n",
    "SLEEP_TIME = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do not change anything below\n",
    "\n",
    "#Facebook often appends some additional stuff to URLs for figuring out how you reached a resource\n",
    "# Let us strip it out\n",
    "def stripParamsFromUrl(url):\n",
    "    scheme, netloc, path, query_string, fragment = urlsplit(url)\n",
    "    return urlunsplit((scheme, netloc, path, '', ''))\n",
    "\n",
    "def get_likes_url(url):\n",
    "    if url[-1] != \"/\":\n",
    "        url = url + \"/\"\n",
    "    if url.find(\"profile.php\") >= 0:\n",
    "        url = url + \"&sk=likes\"\n",
    "    else:\n",
    "        url = url + \"likes\"\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Task t4a\n",
    "def loginToFacebook(driver, user_name, password):\n",
    "    #Change below: Go to facebook.com\n",
    "    driver.dosomething\n",
    "    time.sleep(SLEEP_TIME)\n",
    "    #Change below: Enter the value in user_name variable in the Email text box\n",
    "    emailTextBox = driver.dosomething\n",
    "    emailTextBox.dosomething \n",
    "    #Change below: Enter the value in password variable in the Password text box\n",
    "    passwordTextBox = driver.dosomething\n",
    "    passwordTextBox.dosomething\n",
    "    \n",
    "    passwordTextBox.submit()\n",
    "\n",
    "################REMEMBER TO REMOVE IT BEFORE SUBMITTING#########################\n",
    "loginToFacebook(driver, \"your email id\", \"your password\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#DO not change anything below\n",
    "def goto_profile_page(driver):\n",
    "    #Notice how I get the element using a css selector\n",
    "    elem = driver.find_element_by_css_selector(\"a._2dpe._1ayn\")\n",
    "    elem.click()\n",
    "goto_profile_page(driver)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DO not change anything below\n",
    "def goto_friends_page(driver):\n",
    "    #Notice that here I use a different trick - I use a custom attribute data-tab-key to get friends\n",
    "    # This is preferrable than finding a css selector with weird class names like _2dpe\n",
    "    # The other option is to directly construct the URL of friends tab.\n",
    "    #  But depending on whether your friend has got a custom fb id or not, the url will be different\n",
    "    #  This code keeps the code clean and neat :)\n",
    "    elem = driver.find_element(By.CSS_SELECTOR, \"[data-tab-key='friends']\")\n",
    "    elem.click()    \n",
    "    time.sleep(SLEEP_TIME)\n",
    "\n",
    "#Helper code to get all your friend names and their profile url\n",
    "def get_all_friend_details(driver):\n",
    "    \n",
    "    try:\n",
    "        #Get the friends pagelet. FB pages are organized by pagelets\n",
    "        # Running your find element code within a pagelet is a good idea\n",
    "        pagelet = driver.find_element_by_css_selector(\"#pagelet_timeline_medley_friends > div[id^='collection_wrapper']\")\n",
    "        #Lot of you have hundreds of friends while FB only shows a small subset \n",
    "        # When you scroll down, it loads the remaining friends dynamically\n",
    "        # Find how many friends are their initially\n",
    "        len1 = len(pagelet.find_elements_by_css_selector(\"div.fsl.fwb.fcb > a\"))\n",
    "    except Exception as ex:\n",
    "        print \"Caught exception in getting friends. Try again\"\n",
    "        return []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            #Scroll down\n",
    "            driver.execute_script(\"window.scrollBy(0,10000)\", \"\")\n",
    "            #wait for friend details to load\n",
    "            time.sleep(SLEEP_TIME)\n",
    "            #Find the friends pagelet again\n",
    "            #Both the browser, FB and selenium do aggressive caching\n",
    "            # Sometimes, this might cause invalid references\n",
    "            # Hence, getting the pagelet object fresh is a good idea\n",
    "            pagelet = driver.find_element_by_css_selector(\"#pagelet_timeline_medley_friends > div[id^='collection_wrapper']\")\n",
    "            #Find how many friends you have after scrolling\n",
    "            len2  = len(pagelet.find_elements_by_css_selector(\"div.fsl.fwb.fcb > a\"))\n",
    "            #If it remained the same, we have loaded all of them\n",
    "            # Else repeat the process\n",
    "            if len1 == len2:\n",
    "                break\n",
    "            len1 = len2\n",
    "        except Exception as ex:\n",
    "            break\n",
    "    \n",
    "    #Now we have a page that has all the friends\n",
    "    friends = []\n",
    "    try:\n",
    "        #Get the pagelet object \n",
    "        pagelet = driver.find_element_by_css_selector(\"#pagelet_timeline_medley_friends > div[id^='collection_wrapper']\")\n",
    "        #Get the DOM object containing required details of your friends\n",
    "        all_friends = pagelet.find_elements_by_css_selector(\"div.fsl.fwb.fcb > a\")\n",
    "        if len(all_friends) == 0:\n",
    "            return []\n",
    "        else:\n",
    "            for i in range(len(all_friends)):\n",
    "                #Get their name\n",
    "                name = all_friends[i].get_attribute(\"text\") \n",
    "                #Get their profile url\n",
    "                url = stripParamsFromUrl(all_friends[i].get_attribute(\"href\"))\n",
    "                friends.append( {\"Name\": name, \"ProfileURL\": url})\n",
    "                if i % 100 == 0:\n",
    "                    print \"Handled %s friends\" % (i,)\n",
    "    except Exception as ex:\n",
    "        pass\n",
    "    return friends\n",
    "\n",
    "#Store the list of friends to a file \n",
    "def log_friend_details_to_file(friends_details, file_name):\n",
    "    with open(file_name, \"w\") as output_file:\n",
    "        #Notice how we use json library to convert the array to a string and write to a file\n",
    "        json.dump(friends_details, output_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Do not change anything below\n",
    "#Go to your friends page, collect their details and write it to an output file\n",
    "goto_friends_page(driver)\n",
    "friends_details = get_all_friend_details(driver)\n",
    "log_friend_details_to_file(friends_details, \"fb_friend_dtls.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task t4b: Collect the list of things your friend likes\n",
    "def collect_friend_likes(driver, friend_name, friend_profile_url):\n",
    "    #Directly go to likes tab of the url\n",
    "    likes_url = get_likes_url(friend_profile_url)\n",
    "    driver.get(likes_url)\n",
    "    time.sleep(SLEEP_TIME)\n",
    "    \n",
    "    try:\n",
    "        #Change below: get the likes pagelet\n",
    "        pagelet = None\n",
    "        #Change below: Get the list of items liked currently\n",
    "        len1 = None\n",
    "    except Exception as ex:\n",
    "        #This person has no likes page or has not given us the permission\n",
    "        return []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            driver.execute_script(\"window.scrollBy(0,10000)\", \"\")\n",
    "            time.sleep(SLEEP_TIME)\n",
    "            \n",
    "            #Change below: get the likes pagelet\n",
    "            pagelet = None\n",
    "            #Change below: Get the list of items liked currently\n",
    "            len2  = None\n",
    "            \n",
    "            if len1 == len2:\n",
    "                break\n",
    "            len1 = len2\n",
    "        except Exception as ex:\n",
    "            break\n",
    "    \n",
    "    friend_likes = []\n",
    "    try:\n",
    "        #Change below: get the likes pagelet\n",
    "        pagelet = None\n",
    "        #Change below: Get the list of items liked currently - i.e. get the DOM object with their names\n",
    "        all_friend_likes = None\n",
    "        #Change below: Get the list of items liked currently - i.e. get the DOM object with their type\n",
    "        all_friend_like_types = None\n",
    "        \n",
    "        if len(all_friend_likes) == 0:\n",
    "            return []\n",
    "        else:\n",
    "            for i in range(len(all_friend_likes)):\n",
    "                #Change below: get the name of the item your friend liked. Eg, Bill Gates\n",
    "                like_name = None\n",
    "                #Change below: get the type of the item your friend liked. Eg, Public Figure\n",
    "                like_type = None\n",
    "                \n",
    "                friend_likes.append( {\"Item\": like_name, \"Type\": like_type})\n",
    "    except Exception as ex:\n",
    "        pass\n",
    "    \n",
    "    return friend_likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do not change below\n",
    "#Here, we are going to collect the details of your friends\n",
    "\n",
    "#Get list of friends\n",
    "p4_friend_profile_dtls = json.loads(open(\"fb_friend_dtls.txt\").read())\n",
    "\n",
    "#In case your code crashed when handling the i-th friend, set offset to i. \n",
    "# That way, you will start from that friend instead of starting from scratch\n",
    "p4_offset = 0\n",
    "\n",
    "#Open file in append mode so that prior data if any persists\n",
    "#If you want to start from scratch, remember to delete this file\n",
    "output_file = open(\"fb_friend_like_dtls.txt\", \"a\")\n",
    "for i in range(p4_offset, len(p4_friend_profile_dtls)):\n",
    "    friend_dtls = p4_friend_profile_dtls[i]\n",
    "    friend_name, friend_profile_url = friend_dtls[\"Name\"], friend_dtls[\"ProfileURL\"]\n",
    "    print \"Handling friend %s : %s\" % (i, friend_name)\n",
    "    friend_like_dtls = collect_friend_likes(driver, friend_name, friend_profile_url)\n",
    "    #Append friend_name so that it is findable later\n",
    "    friend_like_dtls = {\"Name\": friend_name, \"Likes\":friend_like_dtls}\n",
    "    json.dump(friend_like_dtls, output_file)\n",
    "    output_file.write(\"\\n\")\n",
    "    output_file.flush()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do not change anything below\n",
    "#This variable has details of your friends in an array\n",
    "# i-th element gives details about i-th friend\n",
    "p4_fb_friend_like_dtls = [json.loads(line) for line in open(\"fb_friend_like_dtls.txt\").readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task t4c:\n",
    "\n",
    "#We now have a list of items your friends liked. But this is in an unstructured format\n",
    "# We need to convert it to a structured format\n",
    "\n",
    "#Step 1: We are going to analyze the data to find the different types of items your friends liked and \n",
    "#   and categorize them accordingly.\n",
    "\n",
    "#Suppose friend1 liked { Bill Gates : Public Figure, Kamal Haasan : Actor/Director} \n",
    "#  and friend2 liked { Bill Gates : Public Figure, Aamir Khan: Public Figure, Sachin Tendulkar : Athlete}\n",
    "# Your code below must produce the following dict\n",
    "#{\n",
    "#    Public Figure: set([Bill Gates, Aamir Khan]),\n",
    "#    Actor/Director: set([Kamal Haasan]),\n",
    "#    Athlete: set([Sachin Tendulkar])\n",
    "#}\n",
    "\n",
    "#We will use a nifty Python package called defaultdict.\n",
    "# See https://docs.python.org/2/library/collections.html#defaultdict-examples for some examples\n",
    "# Here we have instantiatated it so that this is dictionary where the default value type is set\n",
    "t4c_categorized_friend_likes = defaultdict(set)\n",
    "\n",
    "for i in range(len(p4_fb_friend_like_dtls)):\n",
    "    #Change below: set the variable to the list of items liked by i-th friend\n",
    "    #p4_friend_i_likes should now be an array of dictionaries each with two keys: \n",
    "    #  \"Item\": name of the item, \"Type\": the type of item\n",
    "    p4_friend_i_likes = None\n",
    "\n",
    "    \n",
    "    for j in range(len(p4_friend_i_likes)):\n",
    "        p4_friend_i_likes_j_th_entry = p4_friend_i_likes[j]\n",
    "        #Change below: assign it to name of the item\n",
    "        t4c_friend_like_item_name = None\n",
    "        #Change below: assign it to type of the item\n",
    "        t4c_friend_like_item_type = None\n",
    "\n",
    "        #Change below: put each item into appropriate set    \n",
    "        #do something with t4c_categorized_friend_likes, t4c_friend_like_item_type and t4c_friend_like_item_name\n",
    "\n",
    "#Do not change anything below\n",
    "\n",
    "#By default set and dict have no ordering.\n",
    "#But if we want to convert to a vector, we need a particular ordering\n",
    "# So we are going to use them in the sorted order\n",
    "#Sorted list of categories\n",
    "\n",
    "\n",
    "t4_item_categories = sorted(t4c_categorized_friend_likes.keys())\n",
    "print t4_item_categories\n",
    "\n",
    "t4_num_liked_item_categories = len(t4_item_categories)\n",
    "t4_num_liked_items = 0\n",
    "\n",
    "t4_categorized_friend_likes = defaultdict(set)\n",
    "for category in t4_item_categories:\n",
    "    t4_categorized_friend_likes[category] = sorted(t4c_categorized_friend_likes[category])\n",
    "    t4_num_liked_items = t4_num_liked_items + len(t4_categorized_friend_likes[category])\n",
    "    \n",
    "\n",
    "#t4_item_categories: SORTED list of categories such as Actor, Public Figure etc\n",
    "#t4_num_liked_item_categories: number of categories liked\n",
    "#t4_categorized_friend_likes: a dictionary where for each category, you have a SORTED list of items\n",
    "#t4_num_liked_items: number of items all your friends liked overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do not change below\n",
    "#{\n",
    "#    Public Figure: set([Bill Gates, Aamir Khan]),\n",
    "#    Actor/Director: set([Kamal Haasan]),\n",
    "#    Athlete: set([Sachin Tendulkar])\n",
    "#}\n",
    "\n",
    "####Becomes\n",
    "\n",
    "#t4_item_categories = [Actor/Director, Athlete, Public Figure]\n",
    "#t4_categorized_friend_likes = \n",
    "#{\n",
    "#    Actor/Director: set([Kamal Haasan]),\n",
    "#    Athlete: set([Sachin Tendulkar])\n",
    "#    Public Figure: set([Aamir Khan, Bill Gates]),\n",
    "#}\n",
    "\n",
    "#We are now going to convert a tuple of (type, item) into an index\n",
    "# For eg, \n",
    "#    (Actor/Director, Kamal Haasan) => 0\n",
    "#    (Athlete, Sachin Tendulkar) => 1\n",
    "#    (Public Figure, Aamir Khan) => 2\n",
    "#    (Public Figure, Bill Gates) => 3\n",
    "t4_item_category_to_index_dict = {}\n",
    "temp_index = 0\n",
    "for category in t4_item_categories:\n",
    "    for item in t4_categorized_friend_likes[category]:\n",
    "        t4_item_category_to_index_dict[(category, item)] = temp_index\n",
    "        temp_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task t4d\n",
    "#Let us convert each friend to a vector. We will follow a simple binarization technique\n",
    "\n",
    "#Recall that this is our data so far:\n",
    "#t4c_item_categories = [Actor/Director, Athlete, Public Figure]\n",
    "#t4c_categorized_friend_likes = \n",
    "#{\n",
    "#    Public Figure: set([Aamir Khan, Bill Gates]),\n",
    "#    Actor/Director: set([Kamal Haasan]),\n",
    "#    Athlete: set([Sachin Tendulkar])\n",
    "#}\n",
    "\n",
    "#We have four slots - for Aamir Khan, Bill Gates, Kamal Haasan and Sachin Tendulkar respectively\n",
    "# So friend 1 who liked { Bill Gates : Public Figure, Kamal Haasan : Actor/Director} becomes\n",
    "#   becomes [0, 1, 1, 0]\n",
    "# So friend2 who liked { Bill Gates : Public Figure, Aamir Khan: Public Figure, Sachin Tendulkar : Athlete}\n",
    "#   becomes [1, 1, 0, 1]\n",
    "\n",
    "#There are many ways to do it automatically, but it is a good practice to code it up\n",
    "# We will use a fairly inefficient process that is nevertheless easy to understand and code\n",
    "\n",
    "#The three arguments are:\n",
    "#   friend_like_dtls: details of a friend including his/her name and their likes\n",
    "#   item_categories: sorted list of categories\n",
    "#   categorized_friend_likes: a dictionary with item_categories as keys and for key has a sorted list of items\n",
    "#                                  that he/she liked\n",
    "# Output: a vector representation of your friends likes\n",
    "#Hint: use the t4_item_category_to_index_dict variable\n",
    "def t4d_convert_friend_likes_to_vector(friend_like_dtls):\n",
    "    #Initialize vector with all zeros\n",
    "    friend_vector_repr = np.zeros(t4_num_liked_items)\n",
    "    \n",
    "    #Change below: finish the code!\n",
    "    \n",
    "    return friend_vector_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Do not change anything below\n",
    "#Convert each friend to a vector - ignore friends who have no likes or didnt allow us to see it\n",
    "t4_friend_likes_as_vectors = np.array(\n",
    "                                [ t4d_convert_friend_likes_to_vector(friend_like_dtls) \n",
    "                                         for friend_like_dtls in  p4_fb_friend_like_dtls\n",
    "                                         if len(friend_like_dtls[\"Likes\"]) > 0 \n",
    "                                ]\n",
    "                             ) \n",
    "#We now consider the subset of friends with non empty likes\n",
    "#This vector is needed to identify names of your friends from the cluster\n",
    "t4_indices_to_friend_names = [friend_like_dtls[\"Name\"] \n",
    "                             for friend_like_dtls in  p4_fb_friend_like_dtls \n",
    "                             if len(friend_like_dtls[\"Likes\"]) > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task t4e:\n",
    "#Do K-Means clustering on your friends . Call KMeans with default params - but remember to set randomstate to 1234\n",
    "# Remember to send the entire fitted object - not just the labels\n",
    "def t4e_cluster_friends(data, k):\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task 4f:\n",
    "#Vary k from 2 to 20\n",
    "# Call  t4e_cluster_friends with the k.\n",
    "# Plot SSQ, gap statistics and gap differences\n",
    "def t4f_plot_cluster_metrics(data):\n",
    "    pass\n",
    "t4f_plot_cluster_metrics(t4_friend_likes_as_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task 4g\n",
    "#Find optimal k using Gap difference Statistics (i.e find first non zero value)\n",
    "# you can set it by seeing the plot above\n",
    "\n",
    "t4g_opt_k = None\n",
    "t4g_best_clusterings = t4e_cluster_friends(t4_friend_likes_as_vectors, t4g_opt_k )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task 4h\n",
    "#t4g_best_clusterings is a complex object with lot of interesting information\n",
    "# Let us know try to print it in an intuitive way\n",
    "\n",
    "#In the function below, use the clusterings to print the name of your friends in each cluster\n",
    "# The output should look like:\n",
    "# Cluster 1: X, Y,Z\n",
    "# Cluster 2: A,B,C,D etc\n",
    "def t4h_print_cluster_with_friend_names(best_clusterings, indices_to_friend_names):\n",
    "    pass\n",
    "\n",
    "t4h_print_cluster_with_friend_names(t4g_best_clusterings, t4_indices_to_friend_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task 4i\n",
    "#Here is another interesting thing to find from the data\n",
    "# Given a clustering, let us find the set of most representative friends in each cluster\n",
    "# Here is the idea:\n",
    "#   For each cluster:\n",
    "#      find h friends who have the lowest distance to that cluster's centroid and print them\n",
    "\n",
    "def t4i_print_top_representative_friends(best_clusterings, h=1):\n",
    "    pass\n",
    "t4i_print_top_representative_friends(t4g_best_clusterings, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PCA Dimensionality Reduction\n",
    "\n",
    "####################Do not change anything below\n",
    "#Load MNIST data. fetch_mldata will download the dataset and put it in a folder called mldata. \n",
    "#Some things to be aware of:\n",
    "#   The folder mldata will be created in the folder in which you started the notebook\n",
    "#   So to make your life easy, always start IPython notebook from same folder.\n",
    "#   Else the following code will keep downloading MNIST data\n",
    "\n",
    "#Also, if you have downloaded it for PA2, just copy it into this folder and it will work fine.\n",
    "mnist = fetch_mldata(\"MNIST original\", data_home=\".\")       \n",
    "             \n",
    "                                                                                                                            \n",
    " \n",
    "#In order to make the experiments repeatable, we will seed the random number generator to a known value\n",
    "# That way the results of the experiments will always be same\n",
    "np.random.seed(1234)                        \n",
    "\n",
    "#Recall that in PA2 we used shuffle and assigned first 5000 data as training and remaining as testing\n",
    "# Here is another way to do this\n",
    "# Here we are using a function in cross validation module to split \n",
    "# By convention, let us use a 70/30 split\n",
    "p5_train_data, p5_test_data, p5_train_labels, p5_test_labels = \\\n",
    "        train_test_split(mnist.data, mnist.target, test_size=0.3)\n",
    "\n",
    "#The images are in grey scale where each number is between 0 to 255\n",
    "# Now let us normalize them so that the values are between 0 and 1. \n",
    "# This will be the only modification we will make to the image\n",
    "p5_train_data = p5_train_data / 255.0                                        \n",
    "p5_test_data = p5_test_data / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task t5a:\n",
    "# Plot the average value of all digits\n",
    "\n",
    "plt.figure()\n",
    "fig,axes = plt.subplots(2, 5, figsize=(15,4))\n",
    "\n",
    "for i in range(10):\n",
    "    t5a_row, t5a_col = i // 5, i%5\n",
    "    #Change below: Subset p5_train_data with images for digit i only \n",
    "    # Possible to do it 1 liner (similar to how it is done in Pandas)\n",
    "    t5a_digit_i_subset = None\n",
    "\n",
    "    #Change below: compute avg value of t5a_training_data_sevens_only and t5a_training_data_nines_only \n",
    "    # remember to use a vectorized version of mean for efficiency\n",
    "    t5a_digit_i_subset_mean = None\n",
    "\n",
    "    #Do not change below\n",
    "    axes[t5a_row][t5a_col].imshow( t5a_digit_i_subset_mean.reshape(28, 28), cmap=\"Greys\") \n",
    "    axes[t5a_row][t5a_col].grid(False)\n",
    "    axes[t5a_row][t5a_col].get_xaxis().set_ticks([])\n",
    "    axes[t5a_row][t5a_col].get_yaxis().set_ticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task t5b: train a multi class classifier (OneVsRest) with LinearSVC class and make predictions and print it. \n",
    "\n",
    "t5b_start_time = time.time()\n",
    "\n",
    "#Change below: OvR classifier with LinearSVC class with default parameters and random state of 1234\n",
    "t5b_mc_ovr_linear_svc_svm_model = None\n",
    "#Change below: Train the model\n",
    "t5b_mc_ovr_linear_svc_svm_model.fit()\n",
    "print \"SVM training over all features took %s seconds\" % (time.time() - t5b_start_time)\n",
    "\n",
    "#Change below: Make predictions using the model\n",
    "t5b_mc_ovr_predictions_linear_svm_svc = None\n",
    "\n",
    "\n",
    "print \"SVM over all features has an accuracy score of %s\" % (\n",
    "    metrics.accuracy_score(p5_test_labels, t5b_mc_ovr_predictions_linear_svm_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task t5c\n",
    "\n",
    "#Remember that MNIST images are 28x28 => 784 features.\n",
    "#  Often the entire data is not needed and we can find interesting structure in lower dimensions\n",
    "# Let us see how this works\n",
    "#You might want to check http://scikit-learn.org/stable/modules/decomposition.html#decompositions for details\n",
    "\n",
    "\n",
    "#Let us arbitrarily pick number of components as 100\n",
    "t5c_start_time = time.time()\n",
    "#Change below: instantiate PCA object with 100 components\n",
    "t5c_pca = None\n",
    "t5c_pca.fit(p5_train_data)\n",
    "#Change below: transform the training and test class data\n",
    "t5c_train_data_pca = None\n",
    "t5c_test_data_pca = None\n",
    "\n",
    "print \"PCA and transformation took %s seconds\" % (time.time() - t5c_start_time)\n",
    "\n",
    "\n",
    "t5c_start_time = time.time()\n",
    "#Change below: OvR classifier with LinearSVC class with default parameters and random state of 1234\n",
    "t5c_mc_ovr_linear_svc_svm_model = None\n",
    "#Change below: Train the model using the TRANSFORMED training data\n",
    "t5c_mc_ovr_linear_svc_svm_model.fit()\n",
    "print \"SVM training over top-100 components took %s seconds\" % (time.time() - t5c_start_time)\n",
    "\n",
    "#Change below: Make predictions using the model over the TRANSFORMED testing data\n",
    "t5c_mc_ovr_predictions_linear_svm_svc = None\n",
    "\n",
    "\n",
    "\n",
    "print \"SVM over top-100 components has an accuracy score of %s\" % (metrics.accuracy_score(p5_test_labels, t5c_mc_ovr_predictions_linear_svm_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task t5d: Heads up - This is a time consuming task \n",
    "# on my virtual machine with 4gb ram, it took approximately 30 minutes\n",
    "\n",
    "\n",
    "#Means 1,2,3,4,5, 10, 20, 30, 40,, ... 200, 784\n",
    "t5d_num_dimensions_to_test = list(reversed([1,2,3,4,5] + range(10, 200+1, 10) + [784]))\n",
    "\n",
    "#Let us now see how varying number of components affects time and accuracy\n",
    "t5d_columns = [\"Num Components\", \"PCA Time\", \"Training Time\", \"Total Time\", \"Accuracy\"]\n",
    "t5d_results_df = DataFrame(0, index = t5d_num_dimensions_to_test, columns = t5d_columns)\n",
    "\n",
    "for k in t5d_num_dimensions_to_test:\n",
    "    print \"Handling num dimensions = \", k\n",
    "    t5d_start_time = time.time()\n",
    "    \n",
    "    #Change below: instantiate PCA object with k components\n",
    "    t5d_pca = None\n",
    "    \n",
    "    t5d_pca.fit(None)\n",
    "    \n",
    "    #Change below: transform the training and testing class data\n",
    "    t5d_train_data_pca = None\n",
    "    t5d_test_data_pca = None\n",
    "    \n",
    "    t5d_pca_time = time.time() - t5d_start_time\n",
    "    \n",
    "    t5d_start_time = time.time()\n",
    "    #Change below: OvR classifier with LinearSVC class with default parameters and random state of 1234\n",
    "    t5d_mc_ovr_linear_svc_svm_model = None\n",
    "    #Change below: Train the model using the TRANSFORMED training data\n",
    "    t5d_mc_ovr_linear_svc_svm_model.fit()\n",
    "    \n",
    "    t5d_training_time = time.time() - t5d_start_time\n",
    "    \n",
    "    \n",
    "    #Change below: Make predictions using the model over the TRANSFORMED testing data\n",
    "    t5d_mc_ovr_predictions_linear_svm_svc = None\n",
    "    #Change below: Compute the accuracy score\n",
    "    t5d_accuracy = metrics.accuracy_score()\n",
    "\n",
    "    #update df\n",
    "    t5d_results_df.ix[k] = [k, t5d_pca_time, t5d_training_time, \n",
    "                                t5d_pca_time + t5d_training_time, t5d_accuracy]\n",
    "    \n",
    "display(t5d_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task t5e\n",
    "\n",
    "#Let us again consider the model from task 5c where did PCA with 100 components\n",
    "t5e_pca = t5c_pca\n",
    "\n",
    "#As we saw in class, one way to determine how much components is to use\n",
    "# is to set it to the smallest value such that it explained 95% of the variance\n",
    "# Let us how much variance does 100 components explain\n",
    "\n",
    "#Change below: using t5e_pca variable, print the cumulative variance that is explained\n",
    "print \"Total variance explained with 100 components is \", None\n",
    "\n",
    "plt.figure()\n",
    "fig,axes = plt.subplots(1, 2, figsize=(15,4))\n",
    "\n",
    "# Change below: plot the explained variance of these 100 components\n",
    "axes[0].plot(None)\n",
    "axes[0].set_title('Variance Explained by $i$-th Component')\n",
    "\n",
    "# Change below: plot the cumulative explained variance of these 100 components\n",
    "#Hint: numpy has a function for cumulative sum\n",
    "axes[1].plot(None)\n",
    "axes[1].set_title('Variance Explained by top-$i$ Components')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
